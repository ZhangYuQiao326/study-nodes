# 网页基础知识

1. 大部分三部分组成

* HTML（超文本标记语言，定义骨架）
* CSS（层叠样式表，定义外观细节）
* JScript（活动脚本语言，网页交互功能）

2. 常见标签分为两种

* 闭合标签 <head>xx</head>
* 自闭合标签 <xxx...>   没有成对出现的尖括号
* ![image-20221206100625991](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20221206100625991.png)

3. 网络请求/http请求

   ![image-20221206101852794](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20221206101852794.png)

![image-20221206101927707](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20221206101927707.png)

4. 请求头headers

* User-Agent: 用哪个浏览器发送的请求（浏览器种类、版本号等）
* referer：从哪个网页跳转过来的（记录上一个跳转网页地址）
* cookie：标记，用于区分是否是同一个人访问请求

5. requuests.get()

   获取的html本质上为字符串，无法被xpath获取，所以先用lxml进行解析

# Xpath 语法

## 1 节点语法

![image-20221206155745348](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20221206155745348.png)

* 注意：
* /和//的区别，/用来找儿子，//可以直接从爷爷找到孙子





## 2 通配符

![image-20221206161544509](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20221206161544509.png)

特殊情况：

当遇到没有标签的选项的纯文本内容时候，语法：

![image-20221206165507063](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20221206165507063.png)

```
// 选取上一个节点，并确定好其属性值，加上 /following::text()
eg: 
//span[./text()="制片国家/地区："]/following::text()
>>英国/澳大利亚
```







## 3 选取多个路径

| （竖杠）相当于  或 的意思，满足任意任意一个路径都可以有结果

```xml
//metal | //title
```

## 4 找到要爬取的url

1. ![image-20221222112601554](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20221222112601554.png)



2. ![image-20221222112632726](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20221222112632726.png)



```
检查请求url和网址是否一样
若一样，则表示需要爬取的数据在页面源代码里面，可以直接请求获取源代码
```

## 5 快速字典化headers

![image-20221222113735593](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20221222113735593.png)

```
1 clrl+r 调出格式化框
2 勾选正则表达式
3 输入：
 (.*?):(.*)
 '$1':'$2',
4 全部替换，tab键缩进位置，=========注意复制的时候，字符串内有空格============
5 常常只用加下面四个
  cookie host referer  User-Agent
  检查复制后是否有中文，若有的化，需要对应的coookie.encode('utf8')转化，否则会报错
  
```









# 第一章 客户端渲染

## 1.1 打开网页

```py
from urllib.request import urlopen

url = ' http://www.baidu.com '  #1输入网址
resp = urlopen(url)  #打开网页

with open('mybaidu.html', mode='w', encoding='utf-8') as f:   #保存在mybaidu文件
    f.write(resp.read().decode('utf-8'))  # 通过.read拿到源代码，并将源代码写到文件里
print('ok')


```

## 1.2 web请求分析

* 快捷键 

  显示网页源代码 ctrl+U

1. ==服务器渲染==：

   服务器直接返回数据和html，即源代码里能查到数据（百度）

   ![image-20220818173641078](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20220818173641078.png)

2. ==客户端渲染==：

   在客户端完成骨架和数据的拼接，即源代码中找不到数据（豆瓣）

   ![image-20220818173508233](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20220818173508233.png)

------

* 爬虫的关键：找要数据的 请求url

* 使用抓包工具：F12（检查）-network（工作台）-筛选XHR - 预览找到请求url

  ![image-20220818181508814](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20220818181508814.png)

常用内容

* 常规： 请求url

  ​            请求方式：get（显示查询）/post（隐式查询，修改网页）

  ​            状态代码：200成功，404失败等

* 响应头： cookie  本地字符串数据信息（用户登录信息，反爬token）

* 请求标头：cookie  本地字符串数据信息（用户登录信息，反爬token）

​                           Referer 防盗链（本次请求从哪里来的）

​                           User-Agent 请求发送标识（用什么发送的请求）

## 1.3 requests模块

* 注意请求url的最终访问地址，需要在抓包查询

### 1.3.1 requests.get(url , params, headers，verify)

* 直接在浏览器中输入网址查询的，均使用get方式
* 注意url，？前面为网址，后面是参数，可以用params字典导入

```py
#打开一个网页搜索
import requests    #导入库

name = input('please input your need:')

url = f'https://www.bing.com/search?q={name}'   #注意自己删减网址链接
headers = {
'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.81 Safari/537.36 Edg/104.0.1293.54'
}         #用字典形式传入user-agent，反爬

resp = requests.get(url , headers = headers，verify = False)   #get函数得到网页信息 ,有时候需要关闭安全验证
resp.encoding = 'gbk' #若需要转换编码格式，从查询结果中查找charset


print(resp.text) #原代码打印出来
resp.close()   #关掉请求

```

```py
#打印豆瓣喜剧排名
import requests

url = 'https://movie.douban.com/j/chart/top_list' #分割参数
params = {
'type': '24',
'interval_id': '100:90',
'action': '',
'start': 0,
'limit': 20
}
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.81 Safari/537.36 Edg/104.0.1293.54"
}

resp = requests.get(url, params=params, headers=headers)
print(resp.json())
resp.close()
```

```py
#打印源代码
print(rep.text)

#打印结果
print(rep.json)
```



### 1.3.2 requests.post(url , data)

```py
#百度翻译
import requests

w = input('请输入你要翻译的单词：')
url = 'https://fanyi.baidu.com/sug'    #百度翻译的请求url在sug中
data =  {
    'kw': w                            #post方式需要用kw传参，一般在抓包负载中查询
}

resp = requests.post(url, data=data)

print(resp.json())  #将数据通过json（字典）方式传回
resp.close()


```

# 第二章 服务器渲染

## 2.1 正则表达式

在线工具：oschina----在线正则表达式

元字符：具有特定含义的字符串

```py
.    匹配除换行符以外的任意字符
\w   匹配数字，字母，下划线
\d   匹配数字
\n   匹配一个换行符
\s   匹配任意空白符
\t   匹配一个制表符

^    匹配字符串的开始
$    匹配字符串的结束

\W   匹配 非数字，字母，下划线
\D   匹配非数字
\S   匹配非空白符

a|b  匹配字符a或字符b
()   匹配括号内的表达式
[..] 匹配字符组中的字符
[^.] 匹配除了字符组的字符以外的字符
```

量词：控制元字符出现的次数，跟在元字符后

```py
*    重复0次或更多次
+    重复1次或更多次
？   重复0次或1次
{n}  重复n次
{n,} 重复n次或更多次
{n,m} 重复n-m次
```

贪婪与惰性

```py
.*    贪婪匹配
.*?   惰性匹配
```

## 2.2 re模块

* pattern: 正则表达式，一般前面加上==r==转义字符

```py
re.findall(pattern,string)
#返回的是一个列表
import re
list = re.findall(r" \d+ ",'我的收集号码10086，我女朋友手机号12345')
print(list)
['10086','12345']
```

```py
re.finditer(pattern,string)
#返回的是一个迭代器
import re
list = re.finditer(r" \d+ ",'我的收集号码10086，我女朋友手机号12345')
for i in list:       
    print(i.group()) #返回的i是match对象，用group形式返回
```

```py
re.search(patern,string)
#返回匹配的第一个字符串
import re

s = re.search(r'\d+', '我的收集号码10086，我女朋友手机号12345')
print(s.group())    #返回match对象
10086
```

```py
re.match(pattern,string)
#从第一个字符开始查找
import re

string = '123，斯诺4556'
obj = re.compile(r'\d+')
s = obj.match(string)
print(s.group())

```

* 除了findall以外，其余三种均返回match形式，需要group（）返回

```py
# 从一段字符串里面，找一个匹配，用search()
# 从一段字符串里面，找多个匹配，用finditer()
# 从多段字符串里面，分别找一个匹配，先for in遍历每一个字符串，再在for下面search()匹配字符
# 从多段字符串里面，分别找一个匹配，先for in遍历每一个字符串，再在for下面finditer()匹配字符
```



## 2.3 预加载正则表达式或字符串

```py
pat = re.compile(r'pattern',re.S)  #re.S用来确保.也能之别换行符，一般就写上去就行
string = 'xxxx'

s = pat.findall(sring)
```

## 2.4 准确获取具体内容

```py
(?P<分组名字>正则表达式)
#用括号引起来获取信息
```

## 2.5 最终版爬取电影排行

```py

import requests
import re
import csv

#1获取源代码content
url = "https://movie.douban.com/chart"
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.102 Safari/537.36 Edg/104.0.1293.63"
}
rep = requests.get(url, headers=headers)
content = rep.text

#2获取爬虫结果result
pat = re.compile(r' <tr class="item">.*? <a class="nbg".*? title="(?P<name>.*?)">'
                 r'.*?<p class="pl">(?P<time>.*?) /'
                 r'.*?<span class="rating_nums">(?P<score>.*?)</span>'
                 r'.*?<span class="pl">(?P<people>.*?)</span>', re.S)
result = pat.finditer(content)

#3保存数据
with open('result.csv','w',encoding='gbk') as f:
    writer = csv.writer(f)
    writer.writerow(['name', 'time', 'score', 'people'])
    for i in result:
        dic = i.groupdict()
        writer.writerow(dic.values())

#4 显示保存成功
print('over')
```



# 第三章 保存文件

## 3.1 csv格式

CSV 文件又称为逗号分隔值文件，是一种通用的、相对简单的文件格式，用以存储表格数据，包括数字或者字符。

### 3.1.1 以列表形式写入

```py
import csv

#新建my.csv的文件
with open('my.csv','w',encoding='utf-8') as f:
    writer = csv.writer(f)   #使用固定函数csv.writer(),对f进行编写，定义对象writer
    writer.writerrow(['',''])  #导入表头，表头以列表形式保存,通过writer.writerrow函数写入一行
    #导入爬虫结果result在my.csv中
    for i in result:
        dic = i.groupdict()  #先将结果存为字典形式
        
        writer.writerrow(dic.values()) #将字典的值 写入文件
```

### 3.1.2 以字典形式写入

```py
import csv

with open('my.csv', 'w', newline='') as f:
    #构建表头，也就是key
    fieldnames = ['first_name', 'last_name']
    writer = csv.DictWriter(f, fieldnames=fieldnames)  #对f进行编写，关键字参数导入表头
    # 写入字段名，当做表头
    writer.writeheader()  #写入表头
    # 多行写入
    writer.writerows([{'first_name': 'Baked', 'last_name': 'Beans'},{'first_name': 'Lovely', 'last_name': 'Spam'}])
    # 单行写入
    writer.writerow({'first_name': 'Wonderful', 'last_name': 'Spam'})
```

###3.1.3 写操作总结

```py
open with() as f:
    fieldnames = []  #定义表头
    writer = csv.writer(f)            # writer = csv.writer(f, fieldnames=fieldnames)  多一个关键字参数
    wirter.writerrow(fieldnames)      # writer.writerheader()   写表头
    #写内容
```

# 第四章 爬取错层嵌套网页

## 4.1 超链接

* 子网页的网址通常由父网页的网址+子网页的herf

  ![image-20220825200937487](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20220825200937487.png)

* 超链接放在<a  a>中

* 网址拼接

  ```py
  url2 = url1 + i2.group('href').strip('/')
  ```

* 下载链接

  ```py
  #源代码的下载链接在 td style的href里
  <td style="WORD-WRAP: break-word" bgcolor="#fdfddf"><a href="下载链接''
  ```

  

## 4.2 爬取电影天堂下载链接

```py
import requests
import re

# 1.打开第一页面的源代码content1
url1 = 'https://www.dytt89.com/'
resp1 = requests.get(url1)
resp1.encoding = 'gb2312'
content1 = resp1.text

# 2.抓取子网页数据
pat1 = re.compile(r'2022必看热片.*?<ul>(?P<list>.*?)</ul>', re.S)

result1 = pat1.finditer(content1)
for i in result1:
    child_list = i.group('list')

# 3.得到子网页网址合集 url_list
url2_list = []
pat2 = re.compile(r"<a href='(?P<href>.*?)'", re.S)
result2 = pat2.finditer(child_list)
for i2 in result2:
    url2 = url1 + i2.group('href').strip('/')
    url2_list.append(url2)


# 4.打开子网页源代码content2
for i3 in url2_list:
    resp3 = requests.get(i3)
    resp3.encoding = 'gb2312'
    content2 = resp3.text

# 5.爬取下载链接
    pat3 = re.compile(r'◎片　　名(?P<movie>.*?)<br />'
                      r'.*?<td style="WORD-WRAP: break-word" bgcolor="#fdfddf"><a href="(?P<download>.*?)">', re.S)


    result3 = pat3.search(content2)
    print(result3.group("movie"))
    print(result3.group("download"))
```

# 第五章 Bs4

## 5.1 html

* 常见语法

```py
<标签 属性=’属性值‘> 显示值 </标签>

标题 <h1 align='center'>hello</h1>
超链接 <a href='www.'>周杰伦</a>

```

```py
<标签 />    #自带闭合

插入图片 <img scr='...jpg'>
```

# 第六章 xpath解析（服务器渲染）

* 得到源代码的来源

```
1. 直接手动输入进ideal中的一段字符串，要用etree.XML()解析为xml
2. 存在本地的一个网页，用etree.parse()将其导入至ideal中并解析为html
3. 使用request.get()从在线网页爬取源代码为字符串存于ideal中，用etree.HTML()解析为源代码
```

## 5.1 xml

```py
text = """
<book>
    <id>1</id>
    <name>野花遍地香</name>
    <price>1.23</price>
    <nick>臭豆腐</nick>
    <author>
        <nick id="10086">周大强</nick>
        <nick id="10010">周芷若</nick>
        <nick class="joy">周杰伦</nick>
        <nick class="jolin">蔡依林</nick>
        <div>
            <nick>热热热热热1</nick>
        </div>
        <span>
            <nick>热热热热热2</nick>
                <nick>热热热热热3</nick>
        </span>
    </author>

    <partner>
        <nick id="ppc">胖胖陈</nick>
        <nick id="ppbc">胖胖不陈</nick>
    </partner>
</book>
"""
# xml为节点信息
```

```py
from lxml import etree
# 解析是指将网页request提取的源代码字符穿解析为源代码，供给xpath使用
# 直接加载的源代码（输入在代码框）用tree.xml()解析
# 直接从网页中获取的源代码，用tree.html()解析
# 将源代码封装入文件中，可以直接提取文件，tree.parse()解析
xml = etree.XML( text )
# print(xml) <Element xml at 0x2b41c65f680>  存放在寄存器当中，所以要换一种打印方式
result = etree.tostring(xml, encoding="utf8").decode("utf8")
print(result) # 显示为字符串格式
```

## 5.2 本地网页

```py
hello.html 存放位置：C:\Users\zhang\Desktop\hello.html

from lxml import etree
# html = etree.parse(r'C:\Users\zhang\Desktop\hello.html')
# etree自带的解析器为xml，保存本地的网页需要自己定义一个解析器，不然会出现标签丢失的情况
parser = etree.HTMLParser(encoding="utf8") # 定义解析器
html = etree.parse(r'C:\Users\zhang\Desktop\hello.html', parser=parser) # 使用自己的解析器进行解析
```

## 5.3 xpath

注意：：xpath获取道德信息都是保存在列表当中，想要获取具体字符串要用 [0] 索引

拿到标签后的显示内容（注意xpath内要用 "" 括起来，并且用text（）函数获取具体内容）

```py
#从根目录出发/book
tree.xpath('/book')
#找到/bool下所有标签为nick后面的显示内容  
tree.xpath('/book//nick/text()')   #拿到nick节点内的内容   显示内容用/text() 表示

#找到/book/anthor下的div 和 span 下的nick内容
tree.xpath('/book/anthor/*/nick/text()')   #* 表示通配符
#找到/book/anthor 下的所有nick
tree.xpath('/book/anthor//nick/text()')    # //表示所有内容
```

* 具体标签内容的筛选

```py
#1、根据标签顺序索引
#返回的是一个列表！！列表内仅含1个元素

result = tree.xpath('/root[1]/text()')  #标签从1开始
>>[host1]
result = tree.xpath('/root[1]/text()')[0]
>>host1

#2.返回标签值用text()
result = tree.xpath('/root/text()') 

#3、根据标签内属性值索引
<root><herf='www'>xxxxxxx</root>
result = tree.xpath('/root[@href = 'www']/text()')  #用@链接属性，筛选属性值

#4、爬取属性值
result = tree.xpath('/root/@href')
```

* 绝对路径与 相对路径

```py
 <root>
    <author>
        <nick id="10086">周大强</nick>
        <nick id="10010">周芷若</nick>
        <nick class="joy">周杰伦</nick>
        


绝对路径：从根节点出发，/开头，/root/anthor/nick
相对路径：从当前节点出发，./开头， 例如从author出发，./nick
         #./已经替代author，直接从下一节点查找
```

## 5.4 循环处理单个节点

```py
html = etree.HTML(text)
divs = html.xpath('//divs')  # 1.拿到所有的div节点，以==列表==形式返回
divs = html.xpath('//divs')[1:] # 2.除掉第一个div节点，拿到其余的div，以列表形式返回
works = [] # 定义一个列表保存所有节点的爬虫结果
for div in divs:
    # 1.显示每一个节点div
    print(etree.tostring(div, encoding="utf8").decode("utf8")) 
    
    # 2.继续对div节点进行xpath筛选返回列表，用序号索引出对应的字符串
    # 注意相对路径，用 ./进行路径定位，.代替的是循环外的xpath搜索路径
    print(div.xpath(./text()))[0] 
    >>host1
    ********************
    host1
    ********************
    host1
    ********************
    host1
    ********************
    host1
    ********************
    host1
    ********************
    
    # 3.字典中保存每一个div节点的搜索值
    pepole = {}
    name = div.xpath(.name/text())[0]
    age = div.xpath(.age/text())[0]
    people = {
        "name":name,
        "age":age
    }
    >>{'age': 'host1'}
    ********************
    {'age': 'host1'}
    ********************
    {'age': 'host1'}
    ********************
    {'age': 'host1'}
    ********************
    {'age': 'host1'}
    ********************
    {'age': 'host1'}
    
    #4.列表中保存所有的节点值
    works.append(work)
    >>[{'age': 'host1'}, {'age': 'host1'}, {'age': 'host1'}, {'age': 'host1'}, {'age': 'host1'}, {'age': 	'host1'}]

```

##5.5 换页和浏览

```
```

## 5.6 数据保存

```py
# python中
# 对于有很多同一种类型的小格子，经过xpath筛选后，存入字典中,最后每一份的工作汇总到一个列表内
jobs = []
job = {}
# 获取所有格子
all = html.xpath(//xxxx)
# 循环遍历每一个格子
for one in all:
    name = ome.xpath(./xx)
    gender = one,xpath(./xx)
    age = one.xpath(./xx)
    # 存入字典
    job = {
        "name":name,
        "gender":gender,
        "age":age,
    }
    # 汇总入列表中
    jobs.append(job)
```

```py
# 1、存为excel文件语法
import xlwt

# 创建excel
workbook = xlwt.Wookbook(encoding="utf8")
sheet1 = wookbook.add_sheet("sheet_name")

# 写入标题头
sheet1.write(0,0,"name")
sheet1.write(0,1,"age")
sheet1.write(0,2,"gender")

# 往excel中写入内容（比如写入随机数）
for row in range(1,21):
    for col in range(0,3):
        sheet1.write(row,col,random.randint(0,50))

# 注意保存为xls文件，若为xlsx文件，版本过新打不开
workbook.save(r""C:\Users\zhang\Desktop\搜索.xls"")
```

```py
all = []
one = {"keys":value}
#爬虫存入数据模板（通用）
import xlwt
workbook = xlwt.Workbook(encoding="utf8")
# 1.(修改)sheet名字
sheet = workbook.add_sheet("sheet")
# 2.(修改all)取出all中的第一个one对象的键名，并强制转存为列表中，作为小标题
keys = list(all[0].keys()) 
for i in range(len(keys)):
    sheet.write(0, i, keys[i])
# 3.（修改all）从第二行开始存入内容
for row in range(1, len(all)+1, 1): #第二行开始
    for col, key in zip(range(len(keys)), keys):# 同时取出列号、键名
        # 4.(修改all)将all中的第row-1个对象，根据键名key，将value写入对应单元格
        sheet.write(row, col, all[row-1][key]) 

# 4.(修改保存路径和文件名)保存excel
workbook.save(r"C:\Users\zhang\Desktop\搜索.xls")
```

```py
# 2、存为csv文件
csv文件：逗号分隔值，以纯文本形式存储表格数据（也可以设置其他分割符，常用‘|’）
优点：存储文件体积小，传输方便
```

```py
# 将元组写为csv文件
import csv
headers = ['name','age','gender']
values = [('张三'，'80','男')，('张三'，'80','男')，('张三'，'80','男')] #一个元组代表一个信息
with open(r"c://x/x/x",'w',
         newline = '',encoding='utf8') as f:  #删掉标题后的回车键
    writer = csv.writer(f,delimiter='|') # 修改分隔符逗号为管道符
    writer.writerow(headers) # 写入标题
    writer.writerows(values) # 写入内容
    
```

```py
# 将字典写为csv文件
# 创建表头
headers = ['name','score','class']
# 数据
values = [{'name':'张三','score':'20','class':3},
         {'name':'张三','score':'20','class':3},
         {'name':'张三','score':'20','class':3},]

# 创建csv文件
with open(r"c://x/x/x",'w',newline = '',encoding='utf8') as f:
    # 字典的特有写入方式
    writer = csv.DictWriter(f, headers, delimiter='|')
    # 字典的标题头特有的写入方式
    writer.writerheader()
    writer.writerrows(values)

# 创建为csv文件后，转成为excel文件，通过分割列表的方式，将管道符化为单元格
```











## 5.6 爬取豆瓣影评

```py
##第一部分 获取需要爬取的网页链接

import requests
from lxml import etree

# 将网址存到一个列表中
urls = []
for i in range(0,5):
    i = i*20
    url = f'https://movie.douban.com/review/best/?start={i}'
    urls.append(url)
    

cookie = 'll="118318"; bid=tTAAlioHk_Q; __utmz=30149280.1670315049.1.1.utmcsr=baidu|utmccn=(organic)|utmcmd=organic; ap_v=0,6.0; __utma=30149280.752377764.1670315049.1670315049.1671499381.2; __utmb=30149280.0.10.1671499381; __utmc=30149280'
headers = {
    'cookie':cookie,
    'User-Agent':"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36"
    }
```

```py
## 第二部分 拿到每一个页面内具体评论的链接
detail_urls = []
movies = []
i = 0
for url in urls:
    # 发送请求
    response = requests.get(url, headers=headers)
    # 转换编码
    content = response.content.decode('UTF8')
    # 解析html
    html = etree.HTML(content)
    # 拿到一个页面内的每一个影评的url（鼠标点击跳转）#保存在一个列表中，10条数据
    detail_url = html.xpath('//h2/a/@href')[1:]
    # 拿到所有页面的影评url # 保存在列表当中，五个页面，50条数据
    detail_urls.append(detail_url)
```

```py
## 第三部分 对每一页、每一条数据进行爬取
# 对每一页进行解析
for page in detail_urls:
    # 对一页内10条数据解析
    # 防止代码出错，将爬虫代码放在try except continue里面
    try:    
    for url in page:
        response = requests.get(url, headers=headers)
        content = response.content.decode("utf8")
        html = etree.HTML(content)
        # 获取电影信息
        title = html.xpath('//div[@class="subject-title"]/a/text()')[0]
        commenter = html.xpath('//header/a/span/text()')[0]
        scord = html.xpath('//header/span[1]/@title')
        comment = html.xpath('//div[@class="main-bd"]/div/div/p/text()')
        comment = ''.join(comment)
        # 一个字典中存放一个字典内的数据
        movie = {
            "title":title,
            "commenter":commenter,
            "scord":scord,
            "comment":comment
            }
        movies.append(movie)
    except:
        continue      
	i+=1
    print(f'第{i}页爬取完毕！')
```







## 5.5 爬取猪八戒网信息

步骤：

找到对应信息，检查--找到对应源代码--右键复制xpath

```py
import requests
from lxml import etree
# 拿到html源代码并初始化为tree对象(需要修改)
url = 'https://beijing.zbj.com/search/service/?l=0&kw=saas&r=1'
cookie = "your cookie"
headers = {
    'Cookie' = 'cookie',
    'User_Agent' = 'your user_agent'
}
html = requests.get(url=url, headers=headers).text
tree = etree.HTML(html)

```

```
# 一般网页爬取信息，先爬取所有的小方块（找到第一个小方块，把最后的索引去掉），再for循环，查询每一个小方块的内容（相对路径查询）
#拿到所有的公司信息
elems = tree.xpath('//*[@id="__layout"]/div/div[3]/div/div[3]/div[4]/div[1]/div')
#拿到每一个公司的信息
for elem in elems:   （用相对路径，两个绝对路径相减）

    r_price = elem.xpath("./div[3]/div[1]/span/text()")
    price = r_price[0].strip('￥') if len(r_price) == 1 else r_price[1].strip('￥')

    name = elem.xpath("./div[3]/a/text()")

    print(price)
```

```
# 注意！
# 当不用循环遍历，即只查询一次时候，解析出一个大筐筐要用数组的表示方法，即末尾加[0]
elem = tree.xpath('//*[@id="__layout"]/div/div[3]/div/div[3]/div[4]/div[1]/div[1]')[0]
price = elem.xpath("./div[3]/div[1]/span/text()")

```



# 第七章 requests进阶（客户端渲染）

## 7.1session处理登录问题

* 登录账号密码
* 账号密码信息验证成功，通过cookie返回给用户，之后请求带着cookie值进行

```py
import requests

session = requests.session()
url = '登录请求网址'
data = {从f12中获取登录账号密码}

resp = session.post(url,data=data)
```

* 注意开发者工具中的设置勾选项
* ![image-20220830133143165](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20220830133143165.png)

* ![image-20220830133203701](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20220830133203701.png)

* 爬取我的作品

```py
import requests
#登录账号
url = 'https://passport.17k.com/ck/user/login'
data = {
'loginName': '13466937984',
'password' : 'zyq000326'
}

session = requests.session()
session.post(url, data=data)

#通过会话窗口，直接爬取数据
url2 = "https://user.17k.com/ck/author/shelf?page=1&appKey=2406394919"
resp2 = session.get(url2)
print(resp2.json())
```

```py
#法2 直接从作品网址拿数据
url = "https://user.17k.com/ck/author/shelf?page=1&appKey=2406394919"
header = {"cookie":"xxxxxxxxxx"}

resp = requests.get(url,header=header)
print(resp.json())
```

## 7.2 referer解决防盗链问题

* 找请求网址步骤

  （开发者工具中的代码是源代码经过请求后的最终呈现代码）

1. 检查开发者工具中的代码是否存在
2. 若存在，进入源代码中查询对应代码是否存在  ,存在即xpath复制爬取，不存在，说明进行了另外的 js 加载请求处理
3. 从开发者工具中XHR，找到请求返回的网址
4. 对比请求返回网址和最终的网址，一般是将systemtime 和  原网址中cont进行替换

* 防盗链处理

1. get请求网址时候，拿不到数据包，可能进行防盗处理

```py
headers = {"referer":"www.xxx"}
resp = requests.get(url, headers=headers)
```

## 7.3 下载梨视频

```py
# 在原网址拿到cont
url = 'https://www.pearvideo.com/video_1393708'
cont = url.split('_')[1]

# 在请求网址拿到信息包->拿到 srcurl
import requests
r_url = "https://www.pearvideo.com/videoStatus.jsp?contId=1393708&mrd=0.17300234946157356"
headers = {
    "Referer": "https://www.pearvideo.com/video_1393708"
}
resp = requests.get(r_url,headers=headers)

dic = resp.json()
srcUrl = dic["videoInfo"]["videos"]["srcUrl"]
systemTime = dic["systemTime"]
# cont和srcUrl拼接成真实网址
srcUrl = srcUrl.replace(systemTime,f"cont-{cont}")

# 下载视频
with open("video.mp4", mode="wb") as f:
    f.write(requests.get(srcUrl).content)
print('succsess!')
```

## 7.4 代理IP

```py
import requests
#含有代理ip和接口
proxies = {
    "https" : "https://218.60.8.83:3129"
}
resp = requests.get("https://www.baidu.com", proxies=proxies)
resp.encoding = "utf-8"
print(resp.text)
```

# 第八章 线程与进程

## 8.1 多线程

```py
#单线程
def func():
    for i in range(1000):
        print("func", i)


if __name__ == '__main__':
    func()                        #先执行func，在执行main
    for i in range(1000):
        print("main", i)
```

```py
#多线程
from threading import  Thread  #导入自带模块

def func():
    for i in range(1000):
        print("func", i)


if __name__ == '__main__':

    t = Thread(target=func)  #创建多线程t，布置任务target
    t.start()                #t可以执行，具体执行时间根据cpu轮转

    for i in range(1000):
        print("main", i)
```

```py
#多线程方法2  定义一个类
class MyThread(Thread):  #
    def run(self):  # 固定的    -> 当线程被执行的时候, 被执行的就是run()
        for i in range(1000):
            print("子线程", i)


if __name__ == '__main__':
    t = MyThread()
    # t.run()  # 方法的调用了. -> 单线程????
    t.start()  # 开启线程

    for i in range(1000):
        print("主线程", i)
```

## 8.2 多进程

```py
from multiprocessing import Process
def func():
    for i in range(1000):
        print("子进程", i)


if __name__ == '__main__':
    p = Process(target=func)
    p.start()
    for i in range(1000):
        print("主进程", i)
```



# 第九章 常见爬虫错误

## 9.1 编码错误

```py
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 198-199: ordinal not in range(256)
# 请求头headers中含有中文，注意检查cookie、User-Agent中是否含有中文
解决办法:"cookie": cookie.encode("utf8")
```





