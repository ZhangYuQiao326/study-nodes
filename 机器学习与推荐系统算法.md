* sklearn官方中文文档

* ````
  https://sklearn.apachecn.org/#/
  ````

![image-20230314151442517](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230314151442517.png)

# 第一章 绪论

## 1.2 基本术语

* 数据集（dataset）——给定的样本集合，D= {x1，x2...，xm}表示，中间用==逗号==隔开
* 属性（attribute）——d个属性描述
* 属性值（attribute value）
* 属性空间 / 样本空间（attribute space）——根据属性和属性值化成的坐标值，空间内包括所有的组合可能
* 坐标向量——坐标系中的一个点
* 特征向量（feature vector）——一个样本，Xi = （xi1；xi2；..，xid）表示，中间==分号==隔开

* ==注意== ——数据集==一定==属于样本空间，样本空间==不一定==属于数据集
* 学习/训练（learning）——从数据中得到模型的过程，不断逼近真相，分为==监督学习==和==无监督学习==
* 学习器（learner）——生成的模型
* 假设（hypothesis）——模型对应的潜在规律
* 真实/真相（ground-truth）——真实的规律
* 标记（label）——示例结果的信息，用 y 表示
* 标记空间/输出空间（label space）——是所有标记的集合
* 学习任务——分类（classification）  or  回归（regression）

![image-20220928153056617](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20220928153056617.png)

* 聚类

![image-20220928153744774](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20220928153744774.png)

* 泛化（generalization）——做过的题目会，没做过的题目也会
* 

## 1.3 假设空间

* 每一种属性值都+1（代表未知的属性值），构成一个空间，最后在加上一种情况（不存在好瓜），构成假设空间
* 假设空间，即代表所有出现的可能
* 机器学习：在假设空间中寻找符合的样本

# 第二章 模型评估与选择

## 2.1 经验误差和过拟合

1. 错误率
2. 精度 / 正确率 （accuracy）
3. 训练误差 / 经验误差（training error）：学习器在训练集上的误差，==不是越小越好，出现过拟合==
4. 测试误差：学习器在测试集上的误差，==作为泛化误差的近似==
5. 泛化误差（generalization  error）：学习器在新样本上的误差，==无法得到，越小越好==
6. 过拟合 (overfitting).：学习太好了，不属于特点的也归纳，==关键问题，不能解决，只能缓解==
7. 欠拟合 (underfitting)：没有学好，==容易克服==

## 2.2 评估方法（划分训练集和测试集）

### 2.2.1 留出法

1. "留出法" (hold-out) 直接将数据集 划分为两个==互斥==的集合,其中一个 集合作为训练集 ，另一个作为测试集 
2. 训练/测试集的划分要尽可能保持数据分布的一致性，避免 困数据划分过程引入额外的偏差而对最终结果产生影响,==训练 / 测试集要包括所有的知识点==
3. 单次使用留出法得到的估计结果往往不够稳定可靠，在使用留出法时，一般要采用若干次随机划分、==重复进行实验评估后取平均值==作 =为留出法的评估结果.

### 2.2.2 交叉验证法

![image-20220928171354968](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20220928171354968.png)

* 留一法：一个样本作为一折，即拿出一个样本做测试，其余均作训练集

  ​            方法很好，结果准确，但是计算量太大

  ### 2.2.3 自助法

  1. 自助法在数据集较小、难以有效划分训练/测试集时很有用;
  2. 给定包含 个样 本的数据集D， 我们对它进行采样产生数据集 D'
  3. 初始数据集 中约有 36.8% 的样本未出现在采样数据集 D'
  4. ==D' 作为训练集，D 中未被采中的样本作为测试集==
  5. 在初始数据量足够时，留出法和交叉验证法更常用一些

## 2.3 性能度量

1. ==回归==任务常用 ==均方误差==

### 2.3.2 P-R图

* 错误率 和 精度

* 查准率：我挑的好瓜里有多少好瓜（我挑的正例中有多少正例）

  查全率：所有的好瓜有多少被我挑出来（所有的正例中有多少被我挑出）

  混淆矩阵：（以自己的视角为起步看）

  ![image-20221001173805515](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20221001173805515.png)

  ![image-20221001174032863](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20221001174032863.png)

度量F1：综合查准率和查全率，判断算好优劣，默认P,R有同样影响

![image-20221001174907572](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20221001174907572.png)

度量F1一般形式：P，R有不同的影响

![image-20221001174947594](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20221001174947594.png)



### 2.3.3 ROC 与 AUC

* ![image-20221001180810965](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20221001180810965.png)

![image-20221001181019855](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20221001181019855.png)

* 从左下角出发，猜对一个往上走TPR长度，猜错了，往右走FPR距离
* ==TPR图越直线往上，说明该模型算法越好==
* AUC为TPR线下面的面积，面积越大，效果==越好==

### 2.3.4 代价敏感错误率和代价曲线

# 第三章 回归

## 3.1 一元线性回归

一次线性回归（1个一次x）

一个样本对应一个表达式（注意，w是列向量）

==线性回归==的目标是使得线性函数与样本点的==均方差==最小化

利用均方差最小求解w和b

----------

### 3.1.1 最小二乘法

<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230130162416564.png" alt="image-20230130162416564" style="zoom: 33%;" />

---------------------

<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230130162749301.png" alt="image-20230130162749301" style="zoom: 33%;" />

----------------------

解得：

<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20221001200652451.png" alt="image-20221001200652451" style="zoom: 50%;" />

其中，分母可以替换为x方-mxba的平方

<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20221001200714136.png" alt="image-20221001200714136" style="zoom:50%;" />

### 3.1.2 python实现

```py
# 导入数据
import numpy as np
import matplotlib.pyplot as plt
points = np.genfromtxt('data.csv',delimiter=',')
#画出散点图
x = points[:,0]
y = points[:,1]
plt.scatter(x,y)
plt.show
```

<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230130222843121.png" alt="image-20230130222843121" style="zoom:50%;" />

```py
# 损失函数
def compute_cost(points,w,b):
    m = len(points)
    cost = 0
    for i in range(m):
        x = points[i,0]
        y = points[i,1]
        cost += (y-w*x-b)**2
    return cost

# 拟合函数
def average(data):
    sum = 0
    m = len(data)
    for i in range(m):
        sum += data[i]
    return sum/m
            
def fit(points):
    m = len(points)
    x = points[:,0]
    y = points[:,1]
    x_bar = average(x)
    x_up = 0
    x_down1 = 0
    b_left = 0
    for i in range(m):
        x_up += y[i]*(x[i]-x_bar)
        x_down1 += x[i]**2
        
    w = x_up/(x_down1-m*(x_bar**2))
    for i in range(m):
        b_left += y[i]-w*x[i]
    b = b_left/m
    return w,b
```

```py
# 测试
w,b = fit(points)
num = compute_cost(points,w,b)
print('w=',w)
print('b=',b)
print('损失值为',num)
```

<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230130223216501.png" alt="image-20230130223216501" style="zoom: 80%;" />

```py
# 画出最优的拟合曲线
# 散点图
plt.scatter(x,y)
pred_y = w*x+b
# 直线图
plt.plot(x, pred_y, c='r')
plt.show
```

<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230130223259157.png" alt="image-20230130223259157" style="zoom: 80%;" />

## 3.2 多元线性回归



改写线性表达式（x，y看为常数，求解w.b）

m个样本，对应m个表达式



<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20221001200203630.png" alt="image-20221001200203630" style="zoom:50%;" />

解得：

<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20221001202848652.png" alt="image-20221001202848652" style="zoom:50%;" />

<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20221001203652330.png" alt="image-20221001203652330" style="zoom:50%;" />

### 3.2.1 梯度下降

* 开始给定一组sita，给定步长α
* 不断计算梯度，知道迭代为0左右
* sita也不断迭代，直至稳定



* 下图是一次迭代的步骤，即gradit_update的算法

<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230130224510690.png" alt="image-20230130224510690" style="zoom:50%;" />

* 第 j 个sita 梯度计算：第 i 个点的预测值和 y 值相减  x  第 i 个点的第 j 个sita前的系数

* <img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230130235543382.png" alt="image-20230130235543382" style="zoom: 67%;" />

### 3.2.2 python实现

  * 下面代码为一元线性回归的梯度下降方法

  ```py
  # 导入数据
  import numpy as np
  import matplotlib.pyplot as plt
  points = np.genfromtxt('data.csv',delimiter=',')
  x = points[:,0]
  y = points[:,1]
  ```

  ```py
  # 损失函数 ，与最小二乘法一样
  # 损失函数是系数的函数，另外还要传入数据的x，y
  def compute_cost(w, b, points):
      total_cost = 0
      M = len(points)
      
      # 逐点计算平方损失误差，然后求平均数
      for i in range(M):
          x = points[i, 0]
          y = points[i, 1]
          total_cost += ( y - w * x - b ) ** 2
      
      return total_cost/M
  ```

  ```py
  # 梯度下降主函数
  def gradit_desc(points,step,alpha,init_w,init_b):
      w = init_w
      b = init_b
      cost_list = []
      for i in range(step):
          # 每次的损失值用于画图
          cost_list.append(compute_cost(w,b,points))
          # 迭代更新
          w, b = gradit_update(points,init_w,init_b,alpha)
  
      return w, b, cost_list
             
  # 定义更新函数
  def gradit_update(points,w,b,alpha):
      m = len(points)
      b_left = 0
      w_left = 0
      
      for i in range(m):
          x = points[i,0]
          y = points[i,1]
          b_left += w*x+b-y
          w_left += (w*x+b-y)*x
      
      # 更新梯度
      grad_w = 2/m * w_left
      grad_b = 2/m * b_left
      
      # 更新当前本次迭代的参数
      cur_w = w - alpha * grad_w
      cur_b = b - alpha * grad_b
      
      return cur_w, cur_b
  ```

  ```py
  # 自定义超参数
  alpha = 0.0001
  initial_w = 0
  initial_b = 0
  step = 10
  # 测试
  w, b, cost_list = grad_desc( points, initial_w, initial_b, alpha, num_iter )
  
  print("w is: ", w)
  print("b is: ", b)
  
  cost = compute_cost(w, b, points)
  print("cost is: ", cost)
  
  ```

  ```py
  # 画损失函数下降图
  plt.plot(cost_list) #默认以下表为横轴
  plt.show()
  
  # 画出拟合图
  pred_y = w * x + b
  
  plt.scatter(x, y)
  plt.plot(x, pred_y, c='r')
  plt.show()
  ```

  ![image-20230131011142329](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230131011142329.png)

  ![image-20230131011158133](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230131011158133.png)

### 3.2.3 sklearn库

```
# 导入数据
# 定义损失函数
```

```py
# 从线性模型中导入线性回归模型
from sklearn.linear_model import LinearRegression
# 实例化
lr = LinearRegression()

# 模型要求传参为二维数组
x_new = x.reshape(-1, 1)
y_new = y.reshape(-1, 1)
# 拟合
lr.fit(x_new, y_new)

# 从训练好的模型中提取w和b，为列表，用索引拿出具体值
w = lr.coef_[0][0]
b = lr.intercept_[0]
```



分类问题：

* 逻辑回归、朴素贝叶斯、支持向量机、k-邻近、树**

* 评测：精确率和召回率/（查准率和查全率）

  查准率：预测正确的真/预测的所有真

  查全率：预测正确的真/所有真

回归问题==拟合函数曲线

* 线性回归等
* 评测：损失函数 -- 常用平方损失函数 -- 求解法有  最小二乘法/梯度下降/牛顿法

# 第四章 分类

## 4.1 k-近邻算法（knn）

<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230131133756838.png" alt="image-20230131133756838" style="zoom:50%;" />

<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230131133723559.png" alt="image-20230131133723559" style="zoom:50%;" />

<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230131133833254.png" alt="image-20230131133833254" style="zoom:50%;" />

## 4.2 逻辑回归

训练集---训练误差（重要性可忽略）--- 得到模型

验证集--- 选择模型 ---获取惩罚项（正则化项）， 解决过拟合问题---奥卡姆剃刀原则，选择简单的模型，模型越复杂，惩罚项越高，模型越差

测试集 --- 选择最优模型---测试误差（决定泛化能力）

### 4.2.1 压缩函数

<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230131173832262.png" alt="image-20230131173832262" style="zoom:50%;" />

<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230131173808342.png" alt="image-20230131173808342" style="zoom:50%;" />

<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230131173903444.png" alt="image-20230131173903444" style="zoom:50%;" />

### 4.2.2 损失函数

<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230131174726020.png" alt="image-20230131174726020" style="zoom:50%;" />

<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230131174944388.png" alt="image-20230131174944388" style="zoom:50%;" />

<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230131175037023.png" alt="image-20230131175037023" style="zoom:50%;" />

<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230131175208174.png" alt="image-20230131175208174" style="zoom:50%;" />

## 4.3 决策树

<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230201210000414.png" alt="image-20230201210000414" style="zoom:50%;" />

# 第五章 无监督学习

* 分为聚类和降维两种

## 5.1 k均值算法（K_means）

* k 指的是将初始数据划分为 k 类

<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230201193417751.png" alt="image-20230201193417751" style="zoom:50%;" />

<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230201193442362.png" alt="image-20230201193442362" style="zoom:50%;" />

# 第六章 推荐系统

## 6.1 分类

非个性化：热门推荐

个性化推荐：

### 6.1.1 人以群分

* 基于人口统计学的推荐与用户画像

<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230201213400934.png" alt="image-20230201213400934" style="zoom:50%;" />

<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230201213602295.png" alt="image-20230201213602295" style="zoom:50%;" />

用户画像

<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230201213800952.png" alt="image-20230201213800952" style="zoom:50%;" />



### 6.1.2 物以类聚（CB）

* 基于内容的推荐与特征工程（CB content-based）

<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230201214152883.png" alt="image-20230201214152883" style="zoom:50%;" />

<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230201214523785.png" alt="image-20230201214523785" style="zoom:50%;" />

特征工程

<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230201214952414.png" alt="image-20230201214952414" style="zoom: 50%;" />

相似度计算

<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230201214810714.png" alt="image-20230201214810714" style="zoom:50%;" />





### 6.1.3 基于协同过滤的推荐（CF）

![image-20230202151851249](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230202151851249.png)



## 6.2 基于近邻的协同过滤

### 6.2.1 基于用户（User-CF）



### 6.2.2 基于物品（Item-CF）

## 6.3 基于模型的协同过滤

### 6.3.1 隐语义模型（LFM）









# 第七章 特征工程

![image-20230201220227248](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230201220227248.png)

## 7.1 类别型

### 7.1.1 归一化

![image-20230201220309063](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230201220309063.png)

![image-20230201220333717](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230201220333717.png)

### 7.1.2 离散化

![image-20230201220453069](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230201220453069.png)

![image-20230201221008088](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230201221008088.png)

![image-20230201220940187](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230201220940187.png)

### 7.1.3 编码化

![image-20230201221240618](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230201221240618.png)

![image-20230201221325679](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230201221325679.png)

![image-20230201221338991](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230201221338991.png)

## 7.2 时间型

![image-20230201222721093](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230201222721093.png)

## 7.3 统计型

![image-20230201222758937](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230201222758937.png)

# 第八章 推荐算法

## 7.1 TF-IDF算法

### 7.1.1 原理

==基于内容推荐==

为保证个性化推荐，根据词条，tf越高，表示匹配度越高，越要推荐

但是防止热度词条霸榜，所有加上惩罚项，词条热度越高，idf越小

该算法用于找到一篇文档的关键词（你的关键词*是否别人也出现该关键词）

* TF-词频：对于某一个文档，某个词占所有词的比例，==一个项目有多个tf==
* IDF-逆文档频率：对于某一个词，文档总数 除以 出现过该词的文档数目，==一个项目只有一个idf==

![image-20230202002917425](C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230202002917425.png)

### 7.1.2 代码实现

```py
import numpy as np
import pandas as pd
```

```py
# 两篇文章
docA = "The cat sat on my bed"
docB = "The dog sat on my knees"

# 1.构建文章
txtA = docA.split(" ")
txtB = docB.split(" ")


# 2.构建词库列表，并集
wordSet = set(txtA).union(set(txtB))
>>[]'The', 'bed', 'cat', 'dog', 'knees', 'my', 'on', 'sat']
```

```py
# 3.词库词频单独统计
# ，列表创建字典，用统计字典来保存词出现的次数
num_wordA = dict.fromkeys( wordSet, 0 )
num_wordB = dict.fromkeys( wordSet, 0 )

# 遍历文档，统计词数
for word in txtA:
    # 关键字对应的值增1
    num_wordA[word] += 1
for word in txtB:
    num_wordB[word] += 1
    
pd.DataFrame([num_wordtA, num_wordB])
```

<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230202002423370.png" alt="image-20230202002423370" style="zoom: 67%;" />

```py
# TF词频统计
# 传入一个文本、以及该文本的词语频率
def TF( num_word, txt ):
    # 用一个字典对象记录tf，把所有的词对应在bow文档里的tf都算出来
    # 最终结果由具体值，所以可以初始化一个空字典
    tfDict = {}
    totall = len(txt)
    
    for word, count in txt.items():
        tfDict[word] = count / totall
    return tfDict

tfA = TF( wordDictA, bowA )
tfB = TF( wordDictB, bowB )

>>tfA{'cat': 0.16666666666666666,
 'on': 0.16666666666666666,
 'sat': 0.16666666666666666,
 'knees': 0.0,
 'bed': 0.16666666666666666,
 'The': 0.16666666666666666,
 'my': 0.16666666666666666,
 'dog': 0.0}
```

```py
# IDF统计
# 传入一个文本列表,包含多个字典
def computeIDF( num_wordList ):
    # 用一个字典对象保存Ni值，后更换为idf结果
    # Ni：包含某次的文档总数
    # 最终结果要Ni自增1，所以要初始化一个值为0的字典
    #用第一个字典的key作为key，初始值为由词频改为0，即初始化好一个字典
    idfDict = dict.fromkeys(num_wordList[0], 0)
    # 总文章数
    N = len(num_wordList)
    import math
    
    # num_word 即为一篇文章的词频统计字典
    for num_word in num_wordList:
        # 遍历字典中的每个词汇，统计Ni
        for word, count in num_word.items():
            # 含有该词，统计上该文档，Ni+1
            if count > 0:
                idfDict[word] += 1
                
    # 已经得到所有词汇i对应的Ni，现在根据公式把它替换成为idf值
    for word, ni in idfDict.items():
        idfDict[word] = math.log10( (N+1)/(ni+1) )
    
    return idfDict

idf = computeIDF( [num_wordA, num_wordB] )
```

```py
# 计算TF-IDF
def TF_IDF( tf, idf ):
    tfidf = {}
    for word in tf:
        tfidf[word] = tf[word] * idf[word]
    return tfidf

tfidfA = computeTFIDF( tfA, idfs )
tfidfB = computeTFIDF( tfB, idfs )

pd.DataFrame( [tfidfA, tfidfB] )
```

<img src="C:\Users\zhang\AppData\Roaming\Typora\typora-user-images\image-20230202010057184.png" alt="image-20230202010057184" style="zoom:80%;" />

```
1 cat bed 关键词
2 knees dog 关键词
```

### 7.2 LFM算法



